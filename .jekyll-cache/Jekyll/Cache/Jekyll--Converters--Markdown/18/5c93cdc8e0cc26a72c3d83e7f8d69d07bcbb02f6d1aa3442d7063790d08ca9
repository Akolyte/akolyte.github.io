I"YS<p>Date: December 31st, 2020
Class: DS 320
Instructor: Marc Rigas</p>

<h3 id="introduction">Introduction</h3>

<p>In this phase of the semester project our objective was to practice using data science techniques to extract and clean data, and then to integrate data using implementations of some of the techniques we learned in class. In part A of phase 2 we cleaned the data, identified missing values, and formally assessed the similarity of the data using instance-based matching. In part B we used FlexMatcher to match columns of different coronavirus datasets together.</p>

<p>Link to code <a href="https://colab.research.google.com/drive/1pVdyLwwS0c9t_tYFYLqR7zmgtD-FK0zn?usp=sharing" target="_blank">here</a>.</p>

<!--more-->

<h3 id="data-description">Data Description</h3>

<p>Our group used a multitude of datasets. We did not use the same data that we used for part 1. For part A we used fast food location datasets, one from data.world which focused on subway restaurant locations, and another from Kaggle which used a variety of fast food restaurant’s locations such as McDonalds, Wendy’s, etc.</p>

<p>We used the drop_duplicates() function on both datasets once they were converted into dataframes, and after comparing the shape of both the original and the dropped duplicate dataframes, the number of records was the same, so we concluded that there were no 
duplicates.</p>

<p><img src="/imgs/schema_matcher/dup1.JPG" alt="Duplicate 1" /> <img src="/imgs/schema_matcher/dup2.JPG" alt="Duplicate 2" /></p>

<p>To determine where the missing values were we used the count function to count the cells in each column that had non-missing values and compared that to the total number of records.</p>

<p><img src="/imgs/schema_matcher/na1.JPG" alt="NA 1" /> <img src="/imgs/schema_matcher/na2.JPG" alt="NA 2" /> <img src="/imgs/schema_matcher/total_records.JPG" alt="Total Records" /></p>

<p>As you can see from the number of records compared to the count of non-missing values, the dataworld dataframe, the url, phone_number, fax, email, website, facebook, twitter, instagram, pinterest, and youtube values all have missing values. Some of these attributes had all missing values, so we decided to remove them. We did not remove the column website because we thought it would be interesting to have later on for the jaccard similarity matching. We removed 12 attributes in total.</p>

<p>In the kaggle dataframe on the other hand the only attribute that had missing values was websites.</p>

<p>We replaced the missing values with an arbitrary value ‘X’, as we thought a random variable would be appropriate as a replacement for missing values.</p>

<p>The attributes for the datasets are as follows.</p>

<p><strong><a href="https://data.world/data-hut/subway-restaurant-location-dataset">Data.world Restaurant Locations</a> - 11 attributes, 25,533 records</strong></p>

<p><strong>address</strong> - the address of the restaurant</p>

<p><strong>city</strong> - the city the restaurant is located in</p>

<p><strong>country</strong> - the country the restaurant is located in</p>

<p><strong>keys</strong> - the API key used to access the information in the entity</p>

<p><strong>latitude</strong> - the latitude location of the restaurant</p>

<p><strong>longitude</strong> - the longitude location of the restaurant</p>

<p><strong>name</strong> - the name of the restaurant</p>

<p><strong>postalCode</strong> - the zipcode of the restaurant</p>

<p><strong>province</strong> - the state the restaurant is located in</p>

<p><strong>websites</strong> - the website url of the website related to the restaurant</p>

<p><strong><a href="https://www.kaggle.com/datafiniti/fast-food-restaurants?select=FastFoodRestaurants.csv">Kaggle Fast Food</a> - 10 attributes, 10,000 records</strong></p>

<p><strong>name:</strong> represent name of the restaurant,</p>

<p><strong>url</strong> - used to find the information for the other attributes such as city, state, etc.</p>

<p><strong>street_address</strong> - is the street address of the restaurant,</p>

<p><strong>city</strong> - is the city the restaurant is located,</p>

<p><strong>state</strong> -  the state the restaurant is located in,</p>

<p><strong>zip_code</strong> - the zip code of the restaurant,</p>

<p><strong>country</strong> - the country in which the restaurant is located in,</p>

<p><strong>phone_number</strong>* - is the phone number of the restaurant,</p>

<p><strong>fax</strong>* - is the fax of the restaurant, email is the email of the restaurant,</p>

<p><strong>website</strong> - is the website of the restaurant,</p>

<p><strong>open_hours</strong> - is the hours the restaurant is open,</p>

<p><strong>latitude:</strong> is the latitude number the restaurant is located at,</p>

<p><strong>longitude</strong> - is the longitude number the restaurant is located at,</p>

<p><strong>facebook</strong>* - is the link for the facebook page of the restaurant,</p>

<p><strong>twitter</strong>* - is the link for the twitter page of the restaurant,</p>

<p><strong>instagram</strong>* - is the link for the instagram page of the restaurant,</p>

<p><strong>pinterest</strong>* - is the link for the pinterest page of the restaurant,</p>

<p><strong>youtube</strong>* - is the link for the youtube page of the restaurant.</p>

<p>Asterisk = Dropped Attributes</p>

<p>For part B we used the John Hopkins repository discussed in the assignment prompt. From the US daily covid cases reports datasets we chose the July 11th and December 2nd cases. We chose days that would have had spikes from major holidays for fun. We then used the time series data from the same prompt and ran flexmatcher on the daily covid cases datasets to train the schema matcher and then did a prediction for the times series data. Since this an instanced based match we do not need the attributes from the time series data for testing, so we dropped them.</p>

<p>The time series data had 3339 records, and 330 attributes. The July 11th data had 58 records and 18 attributes. The Dec 2nd data had the same number of records and attributes compared to the July 11th dataset. There were also no duplicates after dropping the duplicates using the drop_duplicates function. We show this using the number of records before the drop and after the drop. Looking below you can see that they are the same.</p>

<p><img src="/imgs/schema_matcher/shape1.JPG" alt="Shape 1" /> <img src="/imgs/schema_matcher/shape2.JPG" alt="Shape 2" /></p>

<p>Links for the different data are below.</p>

<p><a href="https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/07-11-2020.csv">US Daily Covid Cases July 11th</a></p>

<p><a href="https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/12-02-2020.csv">US Daily Covid Cases Dec 2nd</a></p>

<p><a href="https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv">US Time Series Covid Cases</a></p>

<h3 id="part-a-jaccard-similarity-measure">Part A: Jaccard Similarity Measure</h3>

<p><strong>Methods:</strong></p>

<p>Comparing the column names some are the same such as name, city, latitude, and longitude, totaling to 4. That leaves 7 attributes that are different for dataworld and 6 for kaggle. Some of the names are completely different from their counterparts such as state in the kaggle dataframe compared to province from the kaggle dataframe. Therefore we decided to use instance-based matching for the jaccard.</p>

<p><img src="/imgs/schema_matcher/instance_logic.JPG" alt="Instance Logic" /></p>

<p>Moving along we compared the performance between using the total word, 2-grams, and 3-grams. In order to get grams we had to create a function that could make the tokens which is shown below. The function is changed depending on whether we wish to create 2-gram or 3-gram tokens. The 3-token code is commented out.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 2-gram
</span><span class="k">def</span> <span class="nf">getGrams</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
  <span class="n">grams</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">array</span><span class="p">:</span>
      <span class="c1"># pads the text to show beginning and end of string
</span>      <span class="n">text_padded</span> <span class="o">=</span> <span class="s">"$"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">+</span> <span class="s">"$"</span>
      <span class="c1">#text_padded = "$$" + str(text) + "$$"
</span>      <span class="c1"># traverses through the string and takes n-grams
</span>      <span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_padded</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
      <span class="c1">#length = len(text_padded) - 2
</span>      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
          <span class="n">first</span> <span class="o">=</span> <span class="n">text_padded</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
          <span class="n">second</span> <span class="o">=</span> <span class="n">text_padded</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
          <span class="c1">#third = text_padded[i+2]
</span>          <span class="n">gram</span> <span class="o">=</span> <span class="n">first</span> <span class="o">+</span> <span class="n">second</span> <span class="c1">#+ third
</span>          <span class="n">grams</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">gram</span><span class="p">)</span>
      <span class="c1"># appends gram to list of grams
</span>  <span class="k">return</span> <span class="n">grams</span>
</code></pre></div></div>

<p>An example of the jaccard script for 2-gram tokens with comments explaining the steps are below. The script is the same for 3-gram besides the commented lines being changed with its associated counterpart for the 2-gram. For word tokens the lines with the gram token function are removed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Jaccard Similarity Measure - 2gram
# Take the set of all the tokens 
</span><span class="n">col_vals</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">getGrams</span><span class="p">(</span><span class="n">kaggle_clean</span><span class="p">[</span><span class="s">'address'</span><span class="p">].</span><span class="n">unique</span><span class="p">()))</span>
<span class="n">jaccard_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c1"># Get the jaccard similarity score for each attribute of dataworld compared to address from kaggle and save it
</span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">dataworld_clean</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">ext_col_vals</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">getGrams</span><span class="p">(</span><span class="n">dataworld_clean</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">unique</span><span class="p">()))</span>
    <span class="n">intersection_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">col_vals</span><span class="p">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">ext_col_vals</span><span class="p">))</span>
    <span class="n">union_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">col_vals</span><span class="p">.</span><span class="n">union</span><span class="p">(</span><span class="n">ext_col_vals</span><span class="p">))</span>
    <span class="n">jaccard</span> <span class="o">=</span> <span class="n">intersection_size</span> <span class="o">/</span> <span class="n">union_size</span>
    <span class="n">jaccard_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">jaccard</span><span class="p">)</span>
    <span class="n">names</span><span class="p">[</span><span class="n">jaccard</span><span class="p">]</span> <span class="o">=</span> <span class="n">col</span>
<span class="c1"># Take the maximum similarity score from the jaccard similarity score list
</span><span class="n">maxed</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">jaccard_list</span><span class="p">)</span>
<span class="c1"># if the maxed is not zero print the similarity score, otherwise all of the similarity scores are 0
</span><span class="k">if</span> <span class="n">maxed</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">maxed</span><span class="p">]</span> <span class="o">+</span> <span class="s">' has similarity score '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">maxed</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"All similarity scores are 0"</span><span class="p">)</span>
</code></pre></div></div>

<p>Results:</p>

<p>The results of the attributes of the kaggle dataframe vs the similarity score are shown below. These charts were generated by ggplot in R. The highest similarity score result with the attribute from the dataworld dataset is encoded using color. Note that even if the bar is almost negligible it is not 0 if the color is not encoded to ‘no match’ in the color legend.</p>

<p>3-token similarity measure:</p>

<p><img src="/imgs/schema_matcher/sim_measure_3.png" alt="Similarity Measure 3 Chart" /></p>

<p>2-token similarity measure:</p>

<p><img src="/imgs/schema_matcher/sim_measure_2.png" alt="Similarity Measure 2 Chart" /></p>

<p>Word token similarity measure:</p>

<p><img src="/imgs/schema_matcher/sim_measure_word.png" alt="Similarity Measure Word Chart" /></p>

<p><strong>Conclusion:</strong></p>

<p>From the results we conclude that the word token jaccard performed the best, with eight out of ten attributes in the kaggle dataframe having the highest similarity scores with the most similar attribute in the dataworld dataframe. Out of those eight, four of them had significant similarity scores. I consider a similarity score to be significant if it is greater than 0.125.</p>

<p>The 2 token jaccard performed second best, with three out of ten attributes in the kaggle dataframe having the highest similarity scores with the most similar attribute in the dataworld dataframe. All of them had significant similarity scores. Interesting to note is that address from the kaggle dataframe matched with city instead of street_address. We attribute this to address being mostly made up of letters, and city is an attribute that is mostly letters, so perhaps values for street_address from the dataworld dataframe had more numbers in the address compared to the address from the kaggle dataframe.</p>

<p>The 3 token jaccard performed the worst, with only two out of ten attributes in the kaggle dataframe having the highest similarity scores with the most similar attribute in the dataworld dataframe. The only significant attribute was address, being related to street_address from dataworld.</p>

<p>From our initial thoughts it was surprising that the 2 token and 3 token jaccards performed so poorly in comparison to the word token jaccard similarity measure. We also found it hard to believe that the similarity score for some of the attributes in the 2 and 3 tokens could be 0. One explanation could be that for all of the values of one attribute in kaggle they are all different enough in the corresponding attribute in dataworld so that the similarity score would be 0.</p>

<h3 id="part-b-schema-matcher-using-flexmatcher">Part B: Schema Matcher using FlexMatcher</h3>

<p><strong>Methods:</strong></p>

<p>When we trained the schema matcher we ran into a problem where flexmatcher doesn’t like dealing with numbers as it applies the .lower() function at some point in the method. As such we had to turn the attributes into strings using the .astype(str) function. After this it ran with lots of warnings but still returned some results. We also had to change the version of pandas to 0.25.1 because FlexMatcher uses the ix method from a previous version of pandas. The schema list had the dataframes for daily covid cases from July 11th and December 2nd. We then got the dictionary of those dataframes to put in the mapping list, and then we put them into the FlexMatcher method with parameter sample size set to 500 to train our schema matcher. The code and the results are shown below.</p>

<p><img src="/imgs/schema_matcher/flexmatcher.jpg" alt="Import FlexMatcher" /></p>

<p><strong>Results</strong></p>

<p>After this we ran the schema matcher model on the time series dataset and returned these results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predicted_mapping</span> <span class="o">=</span> <span class="n">fm</span><span class="p">.</span><span class="n">make_prediction</span><span class="p">(</span><span class="n">covid_cases</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">))</span>
<span class="n">predicted_mapping</span>
</code></pre></div></div>

<p><img src="/imgs/schema_matcher/flexmatcher_results.JPG" alt="FlexMatcher Results" /></p>

<p>From looking at FIPS in the covid daily cases schema, it appears to be an incorrect match as the 0’s in the time series schema are from deaths, and FIPS does not refer to the death count per day.</p>

<p>Comparing the values when looking at the time series csvs and the daily cases csvs Active is not a great match, with most values being integers in the tens of thousands range, while the values in time series corresponding to 1001 are all 4 digit integers.</p>

<p>Deaths are correctly matched to the death number columns in time series.</p>

<p>Last_Update is completely incorrect as it is a date and time measurement, while it is matched to a death column in the time series schema.</p>

<p>The same can be said for Case_Fatility Ratio is also incorrectly matched as it is a number close to 1 in the covid daily cases schema while it is matched to a death column.</p>

<p>People_hospitalized has missing values while it is matched to the deaths column so that is also not accurate.</p>

<p>Recovered, Testing_Rate, Total_Test_Results, Incident_Rate, and Confirmed are also matched to a deaths column when it is in the hundreds of thousands range.</p>

<p>Latitude, Long_, Province_State, Country_Region, ISO3, and UID are correctly matched with their counterparts in the time series schema.</p>

<p>Interestingly Hospitilization_Rate from the daily covid cases schema was matched with what appears to be cities from the time series schema.</p>

<p><strong>Conclusion:</strong></p>

<p>From this particular schema match we learned that sometimes the matcher randomly picks things to match, such as the Hospitilization_Rate from the daily covid cases schema matching with the cities. With regards to schema matching in general it is very good at distinguishing unique values in columns from the testing data and correctly matching them to the correct attribute from the training data. The magnitude of a number also does not matter much when matching, as that is not taken into much consideration as they are considered as strings instead of numbers by the schema matcher.</p>

<p>The results were not what we expected, mostly because we were unsure about how some of the attributes from the time series schema would be matched with the daily covid cases schema. Afterwards we observed that while the schema matcher is good at identifying an attribute correctly when it is unique, it struggles when there are multiple columns in the testing data that are similar to one attribute in the training data. In conclusion, we speculate that this is because of the lack of uniquely similar values present in the training schema in comparison to the testing schema.</p>
:ET